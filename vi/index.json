[
{
	"uri": "/vi/1-prerequisite/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong bước chuẩn bị cho workshop này, ta sẽ thực hiện tải và cài đặt các ứng dụng để phục vụ cho mục tiêu của bài như: Jupyter Notebook và Docker Desktop.\nNội dung: Tải Jupyter Notebook Tải Docker Desktop "
},
{
	"uri": "/vi/3-deploy-model/1-create-api/",
	"title": "Khởi tạo API",
	"tags": [],
	"description": "",
	"content": "Khởi tạo API Trong thư mục TIME_SERIES, tạo thêm một thư mục con với tên app, di chuyển tập tin model.joblib vào thư mục này. Sau đó, tạo một tập tin code Python với tên server.py. Tại tập tin code server.py, viết các lệnh sau để gọi những thư viện cần thiết để tạo API. Chúng ta sẽ dùng FastAPI, joblib để đọc mô hình đã được lưu về máy, và dùng numpy để xử lý dữ liệu đầu vào. from fastapi import FastAPI import joblib import numpy as np Viết lệnh sau để tải mô hình lên. #Load model model = joblib.load(\u0026#39;app/model.joblib\u0026#39;) Viết dòng code sau để bắt đầu tạo một app với FastAPI, dùng decorator @app.get và tạo ra lời chào khi người dùng truy cập vào URL. #Create an app using FastAPI app = FastAPI() @app.get(\u0026#39;/\u0026#39;) def reed__root(): return {\u0026#39;message\u0026#39;: \u0026#39;Bitcoin return model API\u0026#39;} Ta sẽ dùng decorator @app.post để tạo một endpoint cho phép API đưa ra dự báo với dữ liệu về các features cho trước. Khởi tạo hàm dự báo predict nhận tham số là dữ liệu về các features dưới dạng dữ liệu dictionary. Dùng thư viện numpy để chuyển dữ liệu về dạng array, dùng hàm reshape để đưa về số chiều 1D. Dùng hàm predict để thực hiện dự báo với mô hình. Chuyển kết quả dự báo về dạng list và xuất ra kết quả cuối cùng ở dạng dictionary. @app.post(\u0026#39;/predict\u0026#39;) def predict(data: dict): \u0026#34;\u0026#34;\u0026#34;Predict the next day return of a given set of features. Args: data (dict): a dictionary containing the features Returns: a dictionary contains the predicted value\u0026#34;\u0026#34;\u0026#34; features = np.array(data[\u0026#39;features\u0026#39;]).reshape(1, -1) predictions = model.predict(features) prediction_list = predictions.tolist() return {\u0026#39;predicted_next_day_return\u0026#39;: prediction_list} Lưu lại tập tin server.py.\nỞ thư mục TIME_SERIES, tạo một tập tin text với tên requirements.txt, đánh tên các thư viện cần để tạo ra API này: scikit-learn, fastapi, numpy, uvicorn, và xgboost.\nLưu lại tập tin requirements.txt. Chúc mừng bạn đã hoàn thành tạo một API server với Python thông qua thư viện FastAPI.\n"
},
{
	"uri": "/vi/3-deploy-model/2-configure-docker-container/1-create-dockerfile/",
	"title": "Khởi tạo Dockerfile",
	"tags": [],
	"description": "",
	"content": "Khởi tạo Dockerfile Dockerfile là một tập tin giúp ta chỉ định các môi trường và điều kiện cho máy chủ có thể chạy được API.\nTrong thư mục TIME_SERIES, tạo một tập tin với tên Dockerfile. Trong tập tin Dockerfile, viết dòng lệnh sau trên cùng để chỉ định image cần dùng. Ở đây, ta sẽ dùng Python:3.11. # Use an official Python runtime as the base image FROM python:3.11 Viết tiếp dòng lệnh sau để tạo một thư mục code trong container của chúng ta và ấn định làm thư mục làm việc (Working Directory). # Set the working directory in the container WORKDIR /code Tiếp theo, ta sẽ cần sao chép các điều kiện trong tập tin requirements.txt đã được tạo ra vào thư mục code ở trên. # Copy the requirements file to the working directory COPY ./requirements.txt /code/requirements.txt Tạo lệnh để thực hiện tải các thư viện cần thiết sau khi sao chép vào container. # Install the required dependencies RUN pip install --no-cache-dir -r /code/requirements.txt Lần này ra lệnh sao chép các tập tin cho mô hình và API trong thư mục app. # Copy the rest of the application code to the working directory COPY ./app /code/app Thực hiện gán một Port để có thể tương tác với container. Ở đây ta chọn cổng 8000. EXPOSE 8000 Cuối cùng, ta sẽ chỉ định command mà container sẽ chạy sau khi được tạo ra. uvicorn là một máy chủ web nhanh, dựa trên ASGI (Asynchronous Server Gateway Interface) dành cho các ứng dụng web Python. app.server:app sẽ chạy tập tin server.py trong thư mục app và chuẩn bị chạy API được gán vào biến app trong tập tin. Khi cài cấu hình \u0026quot;\u0026ndash;host\u0026quot;, \u0026ldquo;0.0.0.0\u0026rdquo;, các thông tin về network của máy chủ sẽ được chia sẻ với container. Tham số cuối cùng \u0026quot;\u0026ndash;port\u0026quot;, \u0026ldquo;8000\u0026rdquo; dùng để chỉ định port mà ta đã xác định ở trên. # Specify the command to run when the container starts CMD [ \u0026#34;uvicorn\u0026#34;, \u0026#34;app.server:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34; ] Lưu lại tập tin này. Chúc mừng bạn đã hoàn thành bước khởi tạo dockerfile.\n"
},
{
	"uri": "/vi/1-prerequisite/1-download-jupyter-notebook/",
	"title": "Tải Jupyter Notebook",
	"tags": [],
	"description": "",
	"content": "Trong bước này, ta sẽ tiến hành tải và cài đặt Jupyter Notebook trên Visual Studio Code. Nếu các bạn chưa có Visual Studio Code, hãy truy cập trang web sau để tải về và tiến hành cài đặt: https://code.visualstudio.com/\nTải Jupyter Notebook trên VSCode Tạo một thư mục mới trên ổ đĩa với tên TIME_SERIES, truy cập VSCode và mở thư mục lên. Ở thanh điều hướng bên trái, chọn mục Extension, nhập Python ô tìm kiếm sau đó click install. Thực hiện thao tác tương tự và tải về Jupyter Notebook. Sau khi đã cài đặt thành công, ta sẽ tạo một file Jupyter Notebook trong thư mục TIME_SERIES với đuôi ipynb, VD: Workshop_1.ipynb. Sau đó, nhấp chuột vào ô Select Kernel ở góc trên phải màn hình, tích chọn vào môi trường của ngôn ngữ Python trong hộp thoại hiện ra. Trên thực tế, ta nên tạo một môi trường Python ảo riêng cho các dự án để có thể dễ dàng quản lý các phiên bản của ngôn ngữ và thư viện đi kèm với mỗi dự án.\nĐến đây, chúng ta đã hoàn thành việc tải và cài đặt Jupyter Notebook, tiếp theo chúng ta sẽ tiến hành tải Docker Desktop.\n"
},
{
	"uri": "/vi/2-build-model/1-download-and-prepare-data/",
	"title": "Thu thập và chuẩn bị dữ liệu",
	"tags": [],
	"description": "",
	"content": "Thu thập dữ liệu Đầu tiên, ta sẽ mở file code Jupyter Notebook được khởi tạo ở bước thực hành trước lên. Sau đó, viết dòng code dưới vào và bấm tổ hợp nút Shift+Enter để chạy. Các lệnh này dùng để gọi các thư viện cần thiết cho việc thu thập và xử lý dữ liệu ban đầu. Cách thức thực hiện như sau: đánh lệnh import, viết tên thư viện (VD: pandas) và thêm tên viết tắt tùy ý (VD: pd) vào sau lệnh as để tiện cho việc thao tác về sau. Ở đây, pandas cung cấp cho ta các cấu trúc dữ liệu và công cụ phân tích dữ liệu mạnh mẽ, đặc biệt là để làm việc với dữ liệu dạng bảng (giống như bảng tính Excel) và chuỗi thời gian. Thư viện matplotlib cho phép ta vẽ các biểu đồ và hình ảnh trực quan từ dữ liệu. Thư viện yfinance là một thư viện Python phổ biến, được sử dụng để truy xuất dữ liệu tài chính từ Yahoo Finance. Bên cạnh đó, chúng ta cũng dùng thư viện NumPy cung cấp các công cụ và hàm toán học để làm việc với mảng (arrays) và ma trận (matrices) nhiều chiều, và datetime cung cấp các lớp để thao tác với ngày tháng và thời gian. # Gọi các thư viện import pandas as pd import yfinance as yf import matplotlib.pyplot as plt from datetime import datetime import numpy as np Nếu môi trường Python của bạn chưa có các thư viện trên, ta tiến hành tải về bằng lệnh pip install và đánh tên thư viện theo sau và chạy đoạn code (VD: pip install pandas). Ta chỉ cần cài thư viện một lần và dùng lệnh import để gọi cho các lần sử dụng sau.\nĐoạn code sau dùng để thu thập dữ liệu từ thư viện Yahoo Finance. Đầu tiên ta thực hiện truy cập vào bộ dữ liệu của giá Bitcoin bằng hàm Ticker và đánh tên của mã tài sản cần truy cập. Ở đây là BTC-USD hay giá Bitcoin theo đồng USD. Sau đó tiến hành lưu dữ liệu vào một bảng dữ liệu pandas bằng hàm history, nhận tham số là khoảng thời gian cần thu thập của dữ liệu. Ở đây, ta chọn max để lấy tất cả dữ liệu mà Yahoo Finance có về giá của đồng tiền này. Hàm drop với tham số columns giúp ta loại bỏ đi các cột của một bảng dữ liệu. Trong trường hợp này, tiến hành bỏ đi hai cột là Dividends và Stock Splits do chúng không chứa thông tin nào về Bitcoin. Sử dụng hàm to_datetime của thư viện pandas, ta chuyển đổi kiểu dữ liệu của các chỉ mục thành dạng thời gian mà ngôn ngữ có thể hiểu được. # Lấy data từ yahoo finance ticker = yf.Ticker(\u0026#39;BTC-USD\u0026#39;) df = ticker.history(period=\u0026#39;max\u0026#39;) df= df.drop(columns=[\u0026#39;Dividends\u0026#39;,\u0026#39;Stock Splits\u0026#39;]) df.index = pd.to_datetime(df.index) df Tiến hành in bảng dữ liệu ra bằng lệnh print(df) hoặc có thể để df không và chạy. Nếu kết quả in ra giống hình dưới thì các bạn đã thành công. Chuẩn bị dữ liệu Đầu tiên, ta sẽ tiến hành khởi tạo biến mục tiêu là độ biến động của giá đóng cửa (Rate of change), tức là giá Close của ngày hôm sau thay đổi bao nhiêu phần trăm so với giá Close của ngày liền trước đó. Dùng hàm pct_change lên cột Close của bảng dữ liệu trên ta tính được chuỗi dữ liệu mới về độ biến động của Close, lưu thành dạng dữ liệu chuỗi của pandas bằng hàm Series gọi từ thư viện này và đặt tên ROC. Sau đó, ta đưa cột biến mục tiêu này vào lại bảng dữ liệu ban đầu bằng hàm insert dưới tên cột là Targets. #Computing rate of change between the close price of 2 consecutive days. ROC = pd.Series(df[\u0026#39;Close\u0026#39;].pct_change()) df.insert(0,\u0026#39;Targets\u0026#39;,ROC) Việc chuyển đổi bài toán về dự báo Độ biến động thay vì dự báo giá sẽ giúp việc xây dựng mô hình trở nên chính xác và khả thi hơn. Vì dữ liệu Độ biến động không chứa yếu tố xu thế (Trend) như là chuỗi giá gốc, nhờ đó ta có thể cho phép mô hình học được những yếu tố khác thay vì chỉ lấy dữ liệu quá khứ để dự báo. Trong thực tế các bài thực hiện dự báo giá thường không sử dụng được mặc dù có sai số sau huấn luyện rất thấp, vì mô hình sẽ lấy giá trị quá khứ gần nhất để dự báo cho thời gian tiếp theo thay vì học các quy luật từ dữ liệu.\nTa có thể thấy bộ dữ liệu về giá Bitcoin theo ngày sẽ có 5 thành phần: Open, High, Low, Close, và Volume hay có nghĩa là giá mở cửa, giá cao nhất trong ngày, giá thấp nhất trong ngày, giá đóng cửa và khối lượng Bitcoin giao dịch trong ngày đó. Vì chúng đều chứa thông tin về giá của Bitcoin nên ta sẽ sử dụng làm các biến độc lập (features). Ở đây ta phải thực hiện kỹ thuật shift để dịch chuyển dữ liệu xuống 1 dòng với các features, vì ta chỉ được phép sử dụng các thông tin trong quá khứ để dự báo cho tương lai. for i in df.columns : # shifting other columns to form features if i != \u0026#39;Targets\u0026#39;: df[i]=df[i].shift(1) Thực hiện lệnh xuất ra bảng dữ liệu, nếu làm đúng, các bạn sẽ nhìn thấy hình ảnh tại cửa sổ kết quả như sau: Chúc mừng các bạn đã hoàn thành bước thu thập và chuẩn bị dữ liệu.\n"
},
{
	"uri": "/vi/",
	"title": "Xây dựng và triển khai mô hình học máy dự báo chuỗi thời gian",
	"tags": [],
	"description": "",
	"content": "Xây dựng và triển khai mô hình học máy dự báo chuỗi thời gian Tổng quan Trong bài thực hành này, chúng ta sẽ thực hiện phân tích một bộ dữ liệu chuỗi thời gian, giá Bitcoin, và xây dựng mô hình học máy giúp dự báo độ biến động của giá trong ngày tiếp theo bằng Jupyter Notebook. Đồng thời, ta cũng sẽ học cách triển khai mô hình bằng Docker và FastAPI. Các bạn có thể truy cập, xem, và tải về các file code của bài thực hành trên trang Github: https://github.com/haiuy/Time_series_workshop .\nLý do bài workshop chọn xử lý dữ liệu giá Bitcoin vì giá của các sản phẩm tài chính dễ thu thập và được cập nhật liên tục. Hơn nữa, dữ liệu giá của các đồng tiền điện tử thường không bị gián đoạn do tết, lễ như chứng khoán nên có thể phản ánh biến động giá tốt hơn.\nJupyter Notebook Jupyter Notebook là một ứng dụng web mã nguồn mở cho phép bạn tạo và chia sẻ tài liệu có chứa mã nguồn trực tiếp, phương trình, biểu đồ và văn bản giải thích. Nó thường được sử dụng trong nghiên cứu, phân tích dữ liệu, học máy, và giảng dạy.\nDocker Docker là một nền tảng phần mềm cho phép bạn tạo, triển khai và quản lý các ứng dụng trong các container. Container là các môi trường nhẹ và độc lập, chứa mọi thứ cần thiết để chạy một ứng dụng, bao gồm mã nguồn, thư viện, và các công cụ phụ thuộc.\nFastAPI FastAPI là một framework web hiện đại và nhanh chóng cho Python, được thiết kế để tạo ra các API (Application Programming Interface) hiệu quả, dễ bảo trì và có khả năng mở rộng cao. FastAPI được phát triển với trọng tâm là hiệu suất và dễ sử dụng, đặc biệt là cho các ứng dụng cần xử lý đồng thời cao hoặc yêu cầu tốc độ thực thi nhanh.\nNội dung: Các bước chuẩn bị Xây dựng mô hình Triển khai mô hình "
},
{
	"uri": "/vi/3-deploy-model/2-configure-docker-container/2-create-docker-container/",
	"title": "Khởi tạo Docker container",
	"tags": [],
	"description": "",
	"content": "Khởi tạo Image Đầu tiên trong cửa sổ giao diện VSCode, ta sẽ mở một terminal với đường dẫn vào thư mục TIME_SERIES. Trong cửa sổ lệnh terminal, gõ dòng code sau để tạo một image với tên bất kỳ. VD: image_name. docker build -t image_name . Kết quả chạy thành công như sau:\nKhởi tạo container Tiếp tục với cửa sổ terminal, ta đánh lệnh sau để tạo một container với tên tùy ý đặt sau lệnh \u0026ndash;name (VD: time_series_ws), khai báo port là 8000:8000, và tên của image (image_name). docker run --name time_series_ws -p 8000:8000 image_name Kết quả trả ra ở cửa sổ lệnh khi thực hiện thành công:\nTa tiến hành mở Docker Desktop đã tải về máy lên, và truy cập vào mục container. Ở đây giao diện sẽ hiển thị container với các cấu hình mà ta vừa tạo. Nhấp chuột vào dãy số ở mục port. Ta sẽ được chuyển sang một trang web với dòng tin nhắn hiển thị {\u0026ldquo;message\u0026rdquo;:\u0026ldquo;Bitcoin return model API\u0026rdquo;}. Lưu ý rằng khi thay đổi các tập tin về server và dockerfile, ta phải thực hiện lại từ bước tạo image.\nChúc mừng bạn đã hoàn thành tạo lập một docker container.\n"
},
{
	"uri": "/vi/1-prerequisite/2-download-docker-desktop/",
	"title": "Tải và cài đặt Docker Desktop",
	"tags": [],
	"description": "",
	"content": "Ở bước này, chúng ta sẽ tải và cài đặt ứng dụng Docker Desktop.\nTải và cài đặt Docker Desktop. Truy cập vào trang web: https://www.docker.com/products/docker-desktop/ . Chọn phiên bản phần mềm phù hợp với hệ điều hành máy tính của các bạn và bấm tải về. Thực hiện cài đặt phần mềm theo cấu hình mặc định, màn hình giao diện sau sẽ hiện ra khi cài đặt thành công. Trường hợp các bạn gặp lỗi: WSL update failed khi cài đặt như trong hình, để tự cập nhật WSL thủ công hãy làm theo chỉ dẫn trên web này : https://learn.microsoft.com/en-us/windows/wsl/install-manual . Chúc mừng bạn đã cài đặt hoàn tất các tài nguyên phục vụ cho workshop của chúng ta.\n"
},
{
	"uri": "/vi/3-deploy-model/2-configure-docker-container/",
	"title": "Tinh chỉnh Docker container",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần này, ta sẽ tạo một Docker container để chạy API đã tạo ra trong bước trước.\nNội dung Khởi tạo Dockerfile Khởi tạo Docker container "
},
{
	"uri": "/vi/2-build-model/2-feature-engineering/",
	"title": "Trích chọn đặc trưng",
	"tags": [],
	"description": "",
	"content": "Chọn biến độ trễ Biến độ trễ hay lag feature là các biến chứa dữ liệu trong quá khứ của biến mục tiêu. Nói cách khác, biến độ trễ cho phép mô hình của chúng ta nhìn vào các giá trị trong quá khứ của biến phụ thuộc để đưa ra dự báo cho giá trị trong tương lai. Để xác định mức độ trễ phù hợp cho một chuỗi thời gian, ta tiến hành vẽ đồ thị tự tương quan (Autocorrelation) và tự tương quan riêng phần (Partial Autocorrelation) cho chuỗi.\nĐể vẽ các đồ thị trên, ta tiến hành gọi ra hai hàm plot_acf và plot_pacf trong thư viện statsmodels. Sau đó tiến hành cấu hình giao diện hình vẽ bằng các hàm của thư viện matplotlib: Hàm figure với tham số figsize để điều chỉnh kích cỡ của biểu đồ, hàm subplot giúp sắp xếp các biểu đồ trong một ô hiển thị, hàm title để đặt tên, dùng tight_layout giúp đảm bảo các biểu đồ không bị chồng lấn, và cuối cùng thực hiện lệnh plt.show() để xuất biểu đồ ra màn hình. Với các hàm plot_acf và plot_pacf, ta truyền dữ liệu input là chuỗi Targets cần phân tích, độ trễ tối đa đặt 20. Biểu đồ sẽ có trục hoành là các độ trễ tương ứng với các giá trị correlation được thể hiện trên trục tung. # Specify lag features of a time series from statsmodels.graphics.tsaplots import plot_acf, plot_pacf plt.figure(figsize=(12, 6)) # ACF plot plt.subplot(121) plot_acf(df[\u0026#39;Targets\u0026#39;].dropna(), lags=20, ax=plt.gca()) plt.title(\u0026#39;Autocorrelation Function (ACF)\u0026#39;) # PACF plot plt.subplot(122) plot_pacf(df[\u0026#39;Targets\u0026#39;].dropna(), lags=20, ax=plt.gca()) plt.title(\u0026#39;Partial Autocorrelation Function (PACF)\u0026#39;) plt.tight_layout() plt.show() Sau khi chạy đoạn code trên, ta sẽ thấy được đồ thị như sau hiện lên trên màn hình. Nhận xét đồ thị, ta có thể kết luận rằng chuỗi dữ liệu của chúng ta không có mối quan hệ tương quan với các giá trị của chính nó trong quá khứ, vì từ độ trễ 1 (giá trị liền trước) trở đi các đường correlation không có ý nghĩa thống kê (không vượt ra ngoài khoảng xanh bám sát trục hoành). Ngoại trừ giá trị ở độ trễ 0 có correlation cao, nhưng ta không thể có được feature này tại thời điểm dự báo. Vì thế, ta sẽ không cần tạo các biến độ trễ trong trường hợp này. Tạo ra các biến dự báo từ kiến thức về lĩnh vực Chỉ số Bid-Ask-Spread là một chỉ số quan trọng trong thị trường tài chính, phản ánh sự chênh lệch giữa giá Bid (giá mua cao nhất mà người mua sẵn sàng trả) và giá Ask (giá bán thấp nhất mà người bán sẵn sàng chấp nhận), Bid-Ask-Spread thường được dùng để dự báo độ biến động của giá. Ta sử dụng dữ liệu về giá cao nhất (High) và giá thấp nhất (Low) để tính toán Bid-Ask-Spread (BAS) bằng đoạn code sau. # Computing Bid-Ask spread as a feature df[\u0026#39;BAS\u0026#39;]=2*(df[\u0026#39;High\u0026#39;]-df[\u0026#39;Low\u0026#39;])/(df[\u0026#39;High\u0026#39;]+df[\u0026#39;Low\u0026#39;]) Tiếp theo, ta sẽ dùng kỹ thuật Trung bình trượt trong 7 ngày của biến Targets để tạo thêm biến dự báo tạm gọi tắt MA_7. Đây là một kỹ thuật thường dùng trong dự báo tài chính, ta sẽ tính trung bình cộng của biến Targets trong 1 tuần, và shift giá trị xuống một dòng để không bị hiện tượng rò rỉ dữ liệu (Data Leakage). # Computing moving average feature df[\u0026#39;MA_7\u0026#39;]= (df[\u0026#39;Targets\u0026#39;].rolling(window=7).sum())/7 df[\u0026#39;MA_7\u0026#39;]=df[\u0026#39;MA_7\u0026#39;].shift(1) Một chỉ báo quan trọng giúp đo độ biến động là Standard deviation, với cú pháp tương tự như MA_7 ta tính được giá trị Standard deviation trong 1 tuần của biến mục tiêu với đoạn code sau. #Computing standard deviation of return df[\u0026#39;std_7\u0026#39;]=df[\u0026#39;Targets\u0026#39;].rolling(window=7).std() df[\u0026#39;std_7\u0026#39;]=df[\u0026#39;std_7\u0026#39;].shift(1) Chọn biến dự báo phù hợp Sau khi đã xây dựng được các biến dự báo, ta sẽ tiến hành đánh giá độ tương quan giữa các biến với biến mục tiêu bằng chỉ số Mutual Information để có thể thực hiện lựa chọn các biến có ý nghĩa. Đầu tiên, ta cần gọi hàm mutual_info_regression từ thư viện scikit-learn. Sau đó loại bỏ các dòng dữ liệu bị khuyết do quá trình tạo biến ở trên gây nên bằng hàm dropna(). Tiến hành tạo một bảng dữ liệu chỉ chứa các biến dự đoán bằng cách bỏ đi cột Targets và lưu vào một bảng tên feature. Cuối cùng, thực hiện tính giá trị Mutual Information, lưu vào một bảng dữ liệu và in ra màn hình. from sklearn.feature_selection import mutual_info_regression df=df.dropna() feature = df.drop(columns=[\u0026#39;Targets\u0026#39;]) mi_score = pd.Series(mutual_info_regression(feature, df[\u0026#39;Targets\u0026#39;]), name=\u0026#34;MI Scores\u0026#34;, index=feature.columns) mi_score Kết quả khi chạy thành công sẽ nhìn như sau:\nĐể dễ dàng so sánh hơn, ta sẽ tạo một hàm vẽ biểu đồ Thanh ngang biểu thị các MI score tính được bằng dòng code sau. def plot_mi_scores(scores): scores = scores.sort_values(ascending=True) width = np.arange(len(scores)) ticks = list(scores.index) plt.barh(width, scores) plt.yticks(width, ticks) plt.title(\u0026#34;Mutual Information Scores\u0026#34;) plt.figure(dpi=100, figsize=(8, 5)) plot_mi_scores(mi_score) Kết quả của tổ hợp lệnh trên như sau:\nNhìn vào biểu đồ, ta thấy các biến std_7, BAS, và Close là các yếu tố quan trọng nhất để có thể dự báo được Targets. Bên cạnh đó, các biến Open, Low, và High không thực sự cho biết nhiều thông tin về biến mục tiêu. Vì vậy, ta tiến hành loại bỏ 3 biến này để tránh gây nên hiện tượng đa cộng tuyến trong hồi quy bằng hàm drop. df.drop(columns=[\u0026#39;Open\u0026#39;, \u0026#39;Low\u0026#39;, \u0026#39;High\u0026#39;], inplace=True) Hãy in ra bảng dữ liệu df. Nếu bạn làm theo các chỉ dẫn, bảng dữ liệu của bạn sẽ có dạng như sau. Như vậy, ta chỉ còn lại 1 biến phụ thuộc và 5 biến dự báo. Chúc mừng các bạn đã hoàn thành phần thực hành Trích chọn đặc trưng.\n"
},
{
	"uri": "/vi/2-build-model/",
	"title": "Xây dựng mô hình học máy",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần workshop này, chúng ta sẽ thực hiện thu thập, xử lý, và phân tích một bộ dữ liệu chuỗi thời gian. Sau đó ta sẽ sử dụng thuật toán XGBoost phổ biến để dự đoán cho giá trị trong tương lai. Cụ thể hơn, ta sẽ tập trung vào các bước từ Data Collection đến Model Evaluation trong quy trình ML được thể hiện trong hình dưới.\nNội dung Thu thập và chuẩn bị dữ liệu Trích chọn đặc trưng Loại bỏ dữ liệu ngoại lai Huấn luyện mô hình XGBoost Đánh giá mô hình "
},
{
	"uri": "/vi/3-deploy-model/3-test-api/",
	"title": "Kiểm tra API",
	"tags": [],
	"description": "",
	"content": "Kiểm tra API Trong trang web đã khởi tạo ở bước trước, trên đường link ở ô tìm kiếm của trình duyệt, ta gõ thêm /docs và nhấn Enter. Tại giao diện web mới hiện ra, nhấp chuột vào mũi tên ở ô post và nhấp chuột vào ô try it out. Ngay dưới mục Request body, ta sẽ truyền dữ liệu các features đầu vào cho việc dự báo dưới dạng dữ liệu dictionary. VD: \u0026ldquo;features\u0026rdquo;: [64333.542969, 1.882768e+10, 0.018053, 0.014010, 0.026733]. Sau đó, bấm vào ô execute. Nếu chạy thành công, kết quả dự báo của mô hình với dữ liệu đầu vào trên sẽ hiển thị ở ô response body. Chúc mừng bạn đã thành công vượt qua phần thực hành này.\n"
},
{
	"uri": "/vi/2-build-model/3-outlier-detection/",
	"title": "Loại bỏ dữ liệu ngoại lai",
	"tags": [],
	"description": "",
	"content": "Khái niệm dữ liệu ngoại lai Dữ liệu ngoại lai là các điểm dữ liệu nằm xa so với phần lớn các điểm dữ liệu khác trong một tập dữ liệu. Chúng có thể là kết quả của sự sai sót trong quá trình thu thập dữ liệu, hoặc có thể là các giá trị thực sự khác biệt do những nguyên nhân đặc biệt.\nKhi xây dựng mô hình học máy, việc loại bỏ các điểm dữ liệu ngoại lai có thể cần thiết vì chúng có thể gây ảnh hưởng tiêu cực đến hiệu suất và độ chính xác của mô hình. Trong phần thực hành này, ta sẽ học qua một vài phương pháp xử lý dữ liệu ngoại lai.\nTìm và xử lý dữ liệu ngoại lai Để bắt đầu, ta sẽ trực quan hóa dữ liệu mục tiêu bằng hàm plot(), tinh chỉnh tham số figsize và đặt tên cho biểu đồ thông qua title. # VIsualizing target variable to eliminate outlier df[\u0026#39;Targets\u0026#39;].plot(figsize=(16,8), title=\u0026#39;Line chart of target variable\u0026#39;) Nhận xét kết quả, ta thấy vào thời điểm đầu năm 2020 có xuất hiện một điểm dữ liệu đột ngột xuống thấp hơn ngưỡng -0.3.\nTiếp theo ta sẽ thực hiện thống kê mô tả biến Targets bằng cách dùng hàm describe(). #Finding the outlier value df[\u0026#39;Targets\u0026#39;].describe() Kết quả thống kê mô tả cho thấy, các giá trị min và max của chuỗi cách khá xa các khoảng phân vị (25% và 75%).\nTạo một dòng code mới, ta sẽ dùng thư viện seaborn để tạo một biểu đồ hộp cho chuỗi Targets bằng các lệnh sau. # Boxplot of Bitcoin return import seaborn as sns plt.figure(figsize=(10, 6)) # Create the boxplot sns.boxplot(y=df[\u0026#39;Targets\u0026#39;]) # Set the title and labels (optional) plt.title(\u0026#39;Boxplot of Bitcoin Returns\u0026#39;) plt.ylabel(\u0026#39;Daily Returns\u0026#39;) # Show the plot plt.show() Biểu đồ hộp giúp ta biểu diễn phân phối của dữ liệu với các khoảng phân vị của chúng, các điểm dữ liệu màu đen nằm ngoài hai đường kẻ ngang sẽ là các dữ liệu ngoại lai vì chúng xuất hiện với tần suất ít và lệch nhiều so với mẫu.\nỞ đây ta thấy có một số lượng lớn các điểm ngoại lai theo biểu đồ. Tuy nhiên, có một điểm dữ liệu thấp nhất nằm hoàn toàn ra khỏi mẫu dữ liệu của chúng ta. Vì Bitcoin có độ biến động giá khá lớn nên chúng ta có thể cân nhắc giữ lại các điểm ngoại lai gần với đa số các giá trị của mẫu.\nVậy ta sẽ chỉ bỏ đi giá trị min của biến Targets và thay thế giá trị này bằng giá trị trung bình. Dòng code dưới đây đi tính giá trị trung bình của Targets, tìm vị trí của giá trị min, và thay thế vào bảng dữ liệu. #Replacing the outlier with mean value mean_return = df[\u0026#39;Targets\u0026#39;].mean() # Find the index of the minimum value in the return series min_index = df[\u0026#39;Targets\u0026#39;].idxmin() # Replace the minimum value with the mean value df.at[min_index, \u0026#39;Targets\u0026#39;] = mean_return Ta sẽ vẽ lại chuỗi dữ liệu sau khi loại bỏ dữ liệu ngoại lai bằng hàm plot() như bước đầu tiên. #Return time series line chart without outlier df[\u0026#39;Targets\u0026#39;].plot(figsize=(16,8), title=\u0026#39;Line chart of target variable\u0026#39;) Nếu thấy giá trị ngoại lai đã được thay thế như hình dưới thì bạn đã thành công vượt qua phần thực hành này.\n"
},
{
	"uri": "/vi/3-deploy-model/",
	"title": "Triển khai mô hình",
	"tags": [],
	"description": "",
	"content": "Tổng quan Trong phần này, ta sẽ học cách đưa mô hình vào sử dụng cho các công việc dự báo. Theo quy trình ML như hình dưới, chúng ta sẽ tập trung vào phần Model deployment và predictions.\nNội dung Khởi tạo API Tinh chỉnh Docker container Kiểm tra API Tự động dự đoán "
},
{
	"uri": "/vi/4-cleaning-resources/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ tiến hành dọn dẹp tài nguyên theo thứ tự sau\nXóa docker container bằng cách truy cập vào Docker Desktop, container, click stop trong mục Actions, và click vào biểu tượng xóa bên cạnh. Xóa image bằng cách chuyển hướng sang tab image, xác định image cần xóa và bấm vào biểu tượng xóa bên cạnh. "
},
{
	"uri": "/vi/2-build-model/4-xgboost-model/",
	"title": "Huấn luyện mô hình XGBoost",
	"tags": [],
	"description": "",
	"content": "Huấn luyện mô hình XGBoost Đầu tiên, ta sẽ chia bộ dữ liệu thành 2 phần Train set và Validation set bằng hàm split của thư viện numpy. Ta sẽ dùng 80% dữ liệu đầu tiên để huấn luyện mô hình và 20% còn lại cho việc điều chỉnh các tham số. Cuối cùng, in ra các chiều của 2 bộ dữ liệu bằng hàm shape để kiểm tra. train_data, test_data = np.split(df, [int(0.8*len(df))]) print(train_data.shape, test_data.shape) Để đưa dữ liệu vào mô hình, ta cần tiếp tục chia mỗi tập dữ liệu trên thành 2 phần là X (chứa các cột của biến dự báo) và y (chứa cột biến phụ thuộc). Tổ hợp lệnh dưới đây chia các tập dữ liệu bằng hàm pop. y_train = train_data.pop(\u0026#39;Targets\u0026#39;) X_train = train_data y_valid = test_data.pop(\u0026#39;Targets\u0026#39;) X_valid = test_data Ta thực hiện gọi hàm XGBRegressor của thư viện xgboost để huấn luyện mô hình và hàm mean_squared_error của thư viện scikit-learn để tính sai số của giá trị dự báo với giá trị thực. from xgboost import XGBRegressor from sklearn.metrics import mean_squared_error Tiếp theo, chúng ta sẽ truyền vào các tham số đặc trưng của mô hình XGBoost thông qua hàm ước lượng XGBRegressor và lưu lại mô hình vào biến model. Tham số objective dùng để xác định hàm mất mát (Loss function) mà mô hình cần tối ưu. Ở đây ta chọn squared error cho bài toán hồi quy. Tham số n_estimators cho phép ta xác định số lượng của các mô hình đơn lẻ trong XGBoost. Vì XGBoost là một thuật toán ensemble, là kỹ thuật kết hợp nhiều mô hình đơn giản (weak learners), thường là các cây quyết định nhỏ, thành một mô hình mạnh hơn. Ở đây ta đặt số lượng các weak learners này là 1000. Learning rate là tham số cho ta đặt trọng số lên từng mô hình đơn lẻ trong XGBoost điều này ảnh hưởng tới tốc độ học của XGBoost nhưng là một phương pháp tốt để tăng độ chính xác và giảm Overfitting. Ta đặt giá trị Learning_rate = 0.05. Sử dụng thêm early_stopping_rounds sẽ cho phép ta thoát khỏi quá trình huấn luyện sớm khi mô hình không thể giảm thêm error trên tập dữ liệu validation giúp tránh overfitting. Ta đặt tham số này bằng 5, tức quá trình sẽ dừng sau 5 vòng nếu error validation không giảm. Tham số random_state giúp ta có thể tái tạo lại quá trình huấn luyện. #Setting parameters model = XGBRegressor(objective=\u0026#39;reg:squarederror\u0026#39;, n_estimators=1000, learning_rate=0.05, early_stopping_rounds=5, random_state=0) Ta thực hiện ước lượng mô hình bằng hàm fit(), truyền vào các biến dự báo và biến phụ thuộc của tập huấn luyện (train set), đồng thời đưa bộ dữ liệu kiểm tra (validation set) vào tham số eval_set, đặt verbose để hiện thị giá trị của hàm mất mát tại mỗi bước huấn luyện. #Training model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose =2) Giao diện khi huấn luyện của XGBoost có dạng như sau:\nChúc mừng các bạn đã hoàn thành huấn luyện mô hình XGBoost.\n"
},
{
	"uri": "/vi/3-deploy-model/4-automate-inference/",
	"title": "Tự động dự đoán",
	"tags": [],
	"description": "",
	"content": "Tự động dự đoán Trong phần workshop này, ta sẽ tạo một chương trình để tự động tương tác với API, tự động gửi dữ liệu features và nhận lại kết quả dự báo.\nTrong thư mục TIME_SERIES, ta tạo một tập tin Python với tên client.py. Đoạn code sau dùng để gọi các thư viện ta cần dùng: json: là một module được tích hợp sẵn, dùng để mã hóa và giải mã dữ liệu JSON (JavaScript Object Notation). requests: là một công cụ mạnh mẽ và dễ sử dụng để gửi các yêu cầu HTTP. import json import requests Ta khởi tạo một biến (data) để lưu trữ input, và một biến (url) để lưu lại đường dẫn của API. data = [[64333.542969, 1.882768e+10, 0.018053, 0.014010, 0.026733], [64178.992188, 2.143059e+10, 0.013952, 0.011276, 0.028999], [64094.355469, 4.253051e+10, 0.073014, 0.012506, 0.028688], [60381.914062, 2.762573e+10, 0.026280, 0.007034, 0.020180], [61175.191406, 3.273115e+10, 0.049908, 0.006023, 0.021483]] url = \u0026#39;http://localhost:8000/predict/\u0026#39; Trên thực tế. dữ liệu đầu vào có thể được lấy và cập nhật liên tục từ một cơ sở dữ liệu. Ở đây, ta lấy ví dụ input là features của 5 ngày bất kỳ.\nTiếp theo, ta sẽ yêu cầu API trả kết quả bằng cách sau Tạo một biến (predictions) ở kiểu dữ liệu list để lưu trữ kết quả. Dùng một biến thứ cấp (record) và vòng lặp for để duyệt qua các phần tử của input. Dùng thêm một biến khác (payload) để chuyển đổi record về dạng dữ liệu json mà API của chúng ta tương tác được. Sử dụng thư viện requests để yêu cầu tới post của API. Cuối cùng dùng hàm append để ghi lại kết quả vào biến predictions. predictions = [] for record in data: payload = {\u0026#39;features\u0026#39;: record} payload = json.dumps(payload) response = requests.post(url, data=payload) predictions.append(response.json()[\u0026#39;predicted_next_day_return\u0026#39;]) #Run code with interactive window print(predictions) Lưu lại tập tin và chạy với cửa sổ tương tác (Interactive window) trong VSCode. Kết quả dự báo khi chạy thành công sẽ hiện thị ở cửa sổ tương tác dưới dạng một danh sách (list) của các giá trị output từ API. Trong workshop này, các bạn đã đi qua các quá trình để xây dựng một mô hình học máy và đưa chúng vào hoạt động.\nCụ thể hơn, ta đã được thực hành qua các kỹ thuật xử lý dữ liệu chuỗi thời gian, mô hình XGBoost, cách xây dựng một API cho mô hình, các công cụ Jupyter Notebook, và Docker, \u0026hellip;\n"
},
{
	"uri": "/vi/2-build-model/5-model-evaluation/",
	"title": "Đánh giá mô hình",
	"tags": [],
	"description": "",
	"content": "Đánh giá mô hình với một mô hình chuẩn (benchmark) Trong phần này, ta sẽ thực hiện so sánh giữa kết quả của mô hình XGBoost chúng ta vừa huấn luyện với kết quả của một mô hình chuẩn, ở đây ta chọn mô hình trung bình trượt 7 ngày (Moving average 7 days) để làm mốc.\nTrước hết ta sẽ dùng hàm predict để dự báo cho các giá trị của biến mục tiêu trong tập dữ liệu kiểm tra (validation set) và lưu lại các giá trị vào biến predictions. Sau đó, ta dùng hàm sqrt của thư viện math cùng với hàm mean_squared_error để tính giá trị sai số RMSE của dữ liệu mà mô hình dự báo và in kết quả ra màn hình. import math # Root mean square error on validation set predictions = model.predict(X_valid) print(\u0026#34;Root mean squared error: \u0026#34; + str(math.sqrt(mean_squared_error(predictions, y_valid)))) Kết quả sai số của mô hình XGBoost trên tập dữ liệu kiểm tra:\nTiếp theo ta tính giá trị sai số của mô hình mốc (MA_7) so với giá trị thực với cùng kỹ thuật thực hiện như trên. Ở đây, mô hình trung bình trượt đã được ta tính sẵn làm biến dự báo nên không cần khởi tạo lại mô hình. #Calculating error of moving average model on validation set rmse = math.sqrt(mean_squared_error(X_valid[\u0026#39;MA_7\u0026#39;], y_valid)) print(\u0026#34;Root mean squared error of moving average model: \u0026#34; + str(rmse)) Kết quả sai số của mô hình trung bình trượt trên tập kiểm tra:\nVậy kết quả của mô hình XGBoost tốt hơn kết quả của mô hình trung bình trượt với giá trị sai số bé hơn.\nTrực quan hóa giá trị dự báo Bên cạnh các giá trị sai số, việc trực quan hóa các giá trị dự báo sẽ giúp ta có cái nhìn toàn diện hơn về các kết quả.\nTa sẽ dùng loại biểu đồ đường quen thuộc để trực quan hóa chuỗi dữ liệu dự báo trên tập dữ liệu kiểm tra, được lưu trong biến predictions. Tiến hành chuyển đổi chuỗi predictions về dạng dữ liệu bảng pandas, ta cài lại các số chỉ vị trí index là index của tập validation set. Đổi tên cột dữ liệu thành prediction với hàm rename. Dùng thư viện plotly với hàm line để vẽ chuỗi predictions sẽ giúp ta có khả năng tương tác với dữ liệu trên đồ thị. #Visualize insample prediction (validation_set) predictions = pd.DataFrame(predictions, index=test_data.index) predictions.rename(columns={0:\u0026#34;prediction\u0026#34;}, inplace=True) fig = px.line(predictions,y=predictions.columns, title=\u0026#39;Insample prediction (Validation set)\u0026#39;) fig.show() Hình ảnh trực quan hóa của dữ liệu dự báo cho thấy đa phần các giá trị là 0.00211, và mô hình cũng cho các kết quả biến động mạnh ở các thời điểm khác.\nDo kết quả của mô hình tốt hơn mô hình chuẩn, ta sẽ lưu lại mô hình để có thể triển khai và vận hành vào hệ thống phục vụ các công việc cần thiết. Dùng thư viện joblib, gọi hàm dump, truyền vào biến lưu mô hình của chúng ta model, và đặt tên file tùy thích (VD: model.joblib). import joblib joblib.dump(model, \u0026#39;model.joblib\u0026#39;) Hãy thực hiện kiểm tra ổ đĩa và xác thực mô hình đã được lưu đúng. Chúc mừng bạn đã hoàn tất phần thực hành đánh giá mô hình.\n"
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]