[
{
	"uri": "/",
	"title": "Building and deploying a time series machine learning model",
	"tags": [],
	"description": "",
	"content": "Building and deploying a time series machine learning model Overall In this practice session, we will analyze a time series dataset, the price of Bitcoin, and build a machine learning model to predict the volatility of the price for the next day using Jupyter Notebook. Additionally, we will learn how to deploy the model using Docker and FastAPI. You can access, view, and download the code files for the practice session on the Github page: https://github.com/haiuy/Time_series_workshop.\nThe reason this workshop focuses on processing Bitcoin price data is because financial product prices are readily available and continuously updated. Furthermore, cryptocurrency price data are not interrupted by holidays or festivals like stock market data, which allows them to better reflect price volatility.\nJupyter Notebook Jupyter Notebook is an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text. It is commonly used in research, data analysis, machine learning, and education.\nDocker Docker is a software platform that allows you to build, deploy, and manage applications in containers. Containers are lightweight, standalone environments that contain everything necessary to run an application, including the code, libraries, and dependencies.\nFastAPI FastAPI is a modern, fast web framework for Python, designed to create efficient, maintainable, and scalable APIs (Application Programming Interfaces). FastAPI is developed with a focus on performance and ease of use, especially for applications that require high concurrency or fast execution speeds.\nContent: Prerequisite Build model Deploy model "
},
{
	"uri": "/3-deploy-model/1-create-api/",
	"title": "Create API",
	"tags": [],
	"description": "",
	"content": "Create API In the TIME_SERIES directory, create a subdirectory named app and move the model.joblib file into this directory. Then, create a Python code file named server.py. In the server.py file, write the following commands to import the necessary libraries for creating the API. We will use FastAPI, joblib to load the saved model, and numpy for handling input data. from fastapi import FastAPI import joblib import numpy as np Write the following command to load the model. #Load model model = joblib.load(\u0026#39;app/model.joblib\u0026#39;) Write the following code to start creating an app with FastAPI, using the @app.get decorator to generate a greeting when users access the URL. #Create an app using FastAPI app = FastAPI() @app.get(\u0026#39;/\u0026#39;) def reed__root(): return {\u0026#39;message\u0026#39;: \u0026#39;Bitcoin return model API\u0026#39;} Use the @app.post decorator to create an endpoint that allows the API to make predictions based on provided features data. Define a prediction function predict that accepts features data in dictionary format. Use numpy to convert the data to an array, and use the reshape function to format it as 1D. Use the predict function to make predictions with the model. Convert the prediction results to a list and return the final result as a dictionary. @app.post(\u0026#39;/predict\u0026#39;) def predict(data: dict): \u0026#34;\u0026#34;\u0026#34;Predict the next day return of a given set of features. Args: data (dict): a dictionary containing the features Returns: a dictionary contains the predicted value\u0026#34;\u0026#34;\u0026#34; features = np.array(data[\u0026#39;features\u0026#39;]).reshape(1, -1) predictions = model.predict(features) prediction_list = predictions.tolist() return {\u0026#39;predicted_next_day_return\u0026#39;: prediction_list} Save the server.py file.\nIn the TIME_SERIES directory, create a text file named requirements.txt and list the libraries required to create this API: scikit-learn, fastapi, numpy, uvicorn, and xgboost.\nSave the requirements.txt file. Congratulations on creating an API server with Python using the FastAPI library.\n"
},
{
	"uri": "/3-deploy-model/2-configure-docker-container/1-create-dockerfile/",
	"title": "Create Dockerfile",
	"tags": [],
	"description": "",
	"content": "Create Dockerfile The Dockerfile is a file that allows us to specify the environment and conditions for the server to run the API.\nIn the TIME_SERIES directory, create a file named Dockerfile. In the Dockerfile, write the following command at the top to specify the base image. Here, we will use Python:3.11. # Use an official Python runtime as the base image FROM python:3.11 Next, write the following command to create a code directory inside our container and set it as the Working Directory. # Set the working directory in the container WORKDIR /code We will then copy the dependencies from the requirements.txt file created earlier into the code directory. # Copy the requirements file to the working directory COPY ./requirements.txt /code/requirements.txt Add a command to install the necessary libraries after copying them into the container. # Install the required dependencies RUN pip install --no-cache-dir -r /code/requirements.txt Next, copy the model and API files from the app directory. # Copy the rest of the application code to the working directory COPY ./app /code/app Assign a Port so that we can interact with the container. Here, we choose port 8000. EXPOSE 8000 Finally, specify the command that the container will run once itâ€™s created. uvicorn is a fast web server based on ASGI (Asynchronous Server Gateway Interface) for Python web applications. app.server:app will run the server.py file in the app directory and set up the API assigned to the app variable in the file. By configuring \u0026quot;\u0026ndash;host\u0026quot;, \u0026ldquo;0.0.0.0\u0026rdquo;, the network information of the host machine will be shared with the container. The final parameter \u0026quot;\u0026ndash;port\u0026quot;, \u0026ldquo;8000\u0026rdquo; is used to specify the port we defined earlier. # Specify the command to run when the container starts CMD [ \u0026#34;uvicorn\u0026#34;, \u0026#34;app.server:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34; ] Save this file. Congratulations on completing the Dockerfile setup step.\n"
},
{
	"uri": "/2-build-model/1-download-and-prepare-data/",
	"title": "Download and prepare data",
	"tags": [],
	"description": "",
	"content": "Download data First, we\u0026rsquo;ll open the Jupyter Notebook file that was created in the previous exercise. Then, type the following code and press Shift+Enter to run it. These lines are used to import the necessary libraries for data collection and initial processing. The way to do this is as follows: type the import command, write the library name (e.g., pandas) and add an optional abbreviation (e.g., pd) after the as keyword for easier manipulation later. Here, pandas provides us with powerful data structures and analysis tools, especially for working with tabular data (like Excel spreadsheets) and time series. The matplotlib library allows us to create plots and visualizations from data. The yfinance library is a popular Python library used to retrieve financial data from Yahoo Finance. In addition, we also use the NumPy library, which provides mathematical tools and functions for working with multi-dimensional arrays and matrices, and datetime which provides classes for manipulating dates and times. # Import the libraries import pandas as pd import yfinance as yf import matplotlib.pyplot as plt from datetime import datetime import numpy as np If your Python environment doesn\u0026rsquo;t have these libraries yet, let\u0026rsquo;s install them using the pip install command followed by the library name (e.g., pip install pandas). We only need to install a library once and use the import command to call it in subsequent uses.\nThe following code is used to collect data from the Yahoo Finance library. First, we access the Bitcoin price dataset using the Ticker function and specify the ticker symbol we want to retrieve, which is BTC-USD for the Bitcoin price in US dollars. Next, we store the data in a pandas DataFrame using the history function, passing the time period we want to collect data for as a parameter. Here, we choose max to retrieve all available data on Yahoo Finance for this currency. The drop function with the columns parameter allows us to remove columns from a DataFrame. In this case, we remove the Dividends and Stock Splits columns as they do not contain any relevant information for Bitcoin. Using the to_datetime function from the pandas library, we convert the data type of the indices to a time format that the language can understand. # Retrieving Yahoo finance data ticker = yf.Ticker(\u0026#39;BTC-USD\u0026#39;) df = ticker.history(period=\u0026#39;max\u0026#39;) df= df.drop(columns=[\u0026#39;Dividends\u0026#39;,\u0026#39;Stock Splits\u0026#39;]) df.index = pd.to_datetime(df.index) df Let\u0026rsquo;s print the DataFrame using the command print(df) or simply df and run it. If the output matches the image below, you\u0026rsquo;ve done it successfully. Prepare data First, we\u0026rsquo;ll create a target variable representing the rate of change in the closing price. This is calculated as the percentage change in the Close price from one day to the next. By applying the pct_change function to the Close column of the DataFrame, we obtain a new series representing the rate of change in the Close price. We\u0026rsquo;ll store this series in a pandas Series object named ROC. Then, we\u0026rsquo;ll insert this target variable back into the original DataFrame as a new column named Targets using the insert function. #Computing rate of change between the close price of 2 consecutive days. ROC = pd.Series(df[\u0026#39;Close\u0026#39;].pct_change()) df.insert(0,\u0026#39;Targets\u0026#39;,ROC) Shifting our focus from forecasting the price to forecasting the rate of change can significantly improve the accuracy and feasibility of our model. Since the rate of change data does not contain a trend component like the original price series, our model can learn to identify other patterns and factors that influence price movements, rather than simply relying on past prices to make predictions. In practice, models that directly forecast prices often perform poorly, even with low training errors, because they tend to overfit the data and simply predict the most recent price for future time steps.\nWe can observe that the daily Bitcoin price dataset typically consists of five components: Open, High, Low, Close, and Volume, representing the opening price, the highest price, the lowest price, the closing price, and the trading volume for that day, respectively. Since they all contain information about the Bitcoin price, we can use them as independent variables (features). Here, we need to apply a shift technique to shift the data down by one row for these features, as we are only allowed to use past information to predict the future. for i in df.columns : # shifting other columns to form features if i != \u0026#39;Targets\u0026#39;: df[i]=df[i].shift(1) Execute the command to print the DataFrame. If done correctly, you should see the following image in the output window: Congratulations on completing the data collection and preparation steps.\n"
},
{
	"uri": "/1-prerequisite/1-download-jupyter-notebook/",
	"title": "Download Jupyter Notebook",
	"tags": [],
	"description": "",
	"content": "In this step, we will proceed to download and install Jupyter Notebook on Visual Studio Code. If you do not have Visual Studio Code, please visit the following website to download and install it: https://code.visualstudio.com/\nDownload Jupyter Notebook on VSCode Create a new folder on your drive named TIME_SERIES, access VSCode, and open the folder. In the left navigation bar, select the Extension tab, enter Python in the search box, and then click install. Perform a similar action and download Jupyter Notebook. Once installed successfully, create a Jupyter Notebook file in the TIME_SERIES folder with the ipynb extension, for example: Workshop_1.ipynb. Then, click on the Select Kernel box in the upper right corner of the screen, and select the Python environment from the dialog box that appears. In practice, we should create a separate Python virtual environment for each project to easily manage the versions of the language and libraries associated with each project.\nAt this point, we have completed the download and installation of Jupyter Notebook, and next we will proceed to download Docker Desktop.\n"
},
{
	"uri": "/1-prerequisite/",
	"title": "Prerequisite",
	"tags": [],
	"description": "",
	"content": "Overall In preparation for this workshop, we will download and install applications that support the goals of the session, such as Jupyter Notebook and Docker Desktop.\nContent: Download Jupyter Notebook Download Docker Desktop "
},
{
	"uri": "/2-build-model/",
	"title": "Building a machine learning model",
	"tags": [],
	"description": "",
	"content": "Overall In this workshop, we will conduct data collection, processing, and analysis of a time series dataset. Then, we will utilize the popular XGBoost algorithm to forecast future values. Specifically, we will focus on the steps from Data Collection to Model Evaluation in the ML pipeline as illustrated in the figure below.\nContent Download and prepare data Feature Engineering Outlier detection XGBoost model Model evaluation "
},
{
	"uri": "/3-deploy-model/2-configure-docker-container/",
	"title": "Configure Docker container",
	"tags": [],
	"description": "",
	"content": "Overall In this section, we will create a Docker container to run the API server in the previous step.\nContent Create Dockerfile Create Docker container "
},
{
	"uri": "/3-deploy-model/2-configure-docker-container/2-create-docker-container/",
	"title": "Create Docker container",
	"tags": [],
	"description": "",
	"content": "Create an Image First, in the VSCode interface, we will open a terminal with the path directed to the TIME_SERIES directory. In the terminal window, type the following command to create an image with any name. Example: image_name. docker build -t image_name . The successful result looks like this:\nCreate a container Continuing in the terminal window, type the following command to create a container with a custom name placed after the \u0026ndash;name option (e.g., time_series_ws), declare the port as 8000:8000, and specify the image name (image_name). docker run --name time_series_ws -p 8000:8000 image_name The output in the terminal upon successful execution:\nOpen Docker Desktop that you have downloaded, and navigate to the container section. Here, the interface will display the container with the configurations you just created. Click on the number in the port section. You will be redirected to a webpage displaying the message {\u0026ldquo;message\u0026rdquo;:\u0026ldquo;Bitcoin return model API\u0026rdquo;}. Note that when making changes to the server and dockerfile files, you need to recreate the image from the beginning.\nCongratulations on successfully creating a docker container.\n"
},
{
	"uri": "/1-prerequisite/2-download-docker-desktop/",
	"title": "Download and set up Docker Desktop",
	"tags": [],
	"description": "",
	"content": "In this step, we will download and install the Docker Desktop application.\nDownload and set up Docker Desktop. Visit the website: https://www.docker.com/products/docker-desktop/. Select the appropriate software version for your computer\u0026rsquo;s operating system and click download. Install the software with the default configuration, and the following interface will appear when the installation is successful. In case you encounter the error: WSL update failed during installation as shown in the image, to manually update WSL, please follow the instructions on this website: https://learn.microsoft.com/en-us/windows/wsl/install-manual. Congratulations on successfully installing all the resources needed for our workshop.\n"
},
{
	"uri": "/2-build-model/2-feature-engineering/",
	"title": "Feature Engineering",
	"tags": [],
	"description": "",
	"content": "Lag feature Lag features are variables that contain past values of the target variable. In other words, lag features allow our model to look at past values of the dependent variable to make predictions about future values. To determine the appropriate lag order for a time series, we plot the autocorrelation function (ACF) and partial autocorrelation function (PACF).\nTo plot these graphs, we use the plot_acf and plot_pacf functions from the statsmodels library. We then customize the plot using functions from the matplotlib library: figsize to adjust the figure size, subplot to arrange multiple plots in a figure, title to set the title, tight_layout to prevent overlapping, and finally plt.show() to display the plot. We pass the Targets series to the plot_acf and plot_pacf functions, specifying a maximum lag of 20. The x-axis represents the lags, and the y-axis shows the corresponding correlation values. # Specify lag features of a time series from statsmodels.graphics.tsaplots import plot_acf, plot_pacf plt.figure(figsize=(12, 6)) # ACF plot plt.subplot(121) plot_acf(df[\u0026#39;Targets\u0026#39;].dropna(), lags=20, ax=plt.gca()) plt.title(\u0026#39;Autocorrelation Function (ACF)\u0026#39;) # PACF plot plt.subplot(122) plot_pacf(df[\u0026#39;Targets\u0026#39;].dropna(), lags=20, ax=plt.gca()) plt.title(\u0026#39;Partial Autocorrelation Function (PACF)\u0026#39;) plt.tight_layout() plt.show() After running the code, we observe that the time series does not exhibit significant autocorrelation with its past values, as the correlation values for lags 1 and beyond do not fall outside the blue confidence bands. The only significant correlation is at lag 0, but this feature is not available at prediction time. Therefore, we do not need to create lag features in this case. Features from domain knowledge The Bid-Ask Spread is a crucial indicator in financial markets, reflecting the difference between the Bid price (the highest price a buyer is willing to pay) and the Ask price (the lowest price a seller is willing to accept). The Bid-Ask Spread is often used to forecast price volatility. We use data on the highest price (High) and the lowest price (Low) to calculate the Bid-Ask Spread (BAS) using the following code. # Computing Bid-Ask spread as a feature df[\u0026#39;BAS\u0026#39;]=2*(df[\u0026#39;High\u0026#39;]-df[\u0026#39;Low\u0026#39;])/(df[\u0026#39;High\u0026#39;]+df[\u0026#39;Low\u0026#39;]) Next, we\u0026rsquo;ll apply a 7-day Moving Average to the Targets variable to create a new feature called MA_7. This is a common technique in financial forecasting. We calculate the average of the Targets variable over a 1-week period and then shift the values down by one row to prevent data leakage. # Computing moving average feature df[\u0026#39;MA_7\u0026#39;]= (df[\u0026#39;Targets\u0026#39;].rolling(window=7).sum())/7 df[\u0026#39;MA_7\u0026#39;]=df[\u0026#39;MA_7\u0026#39;].shift(1) Another important indicator of volatility is the Standard deviation. Similar to the MA_7, we calculate the Standard deviation of the target variable over a 1-week period using the following code. #Computing standard deviation of return df[\u0026#39;std_7\u0026#39;]=df[\u0026#39;Targets\u0026#39;].rolling(window=7).std() df[\u0026#39;std_7\u0026#39;]=df[\u0026#39;std_7\u0026#39;].shift(1) Selecting features After constructing the predictive features, we will evaluate the correlation between these features and the target variable using the Mutual Information metric to select meaningful features. First, we call the mutual_info_regression function from the scikit-learn library. Then, we remove rows with missing values caused by the feature creation process using the dropna() function. Next, we create a DataFrame containing only the predictive features by dropping the Targets column and storing it in a DataFrame named feature. Finally, we calculate the Mutual Information values, store them in a DataFrame, and print the results. from sklearn.feature_selection import mutual_info_regression df=df.dropna() feature = df.drop(columns=[\u0026#39;Targets\u0026#39;]) mi_score = pd.Series(mutual_info_regression(feature, df[\u0026#39;Targets\u0026#39;]), name=\u0026#34;MI Scores\u0026#34;, index=feature.columns) mi_score The result of the above lines of code:\nTo facilitate comparison, we will create a function to plot a horizontal bar chart representing the calculated MI scores using the following code. def plot_mi_scores(scores): scores = scores.sort_values(ascending=True) width = np.arange(len(scores)) ticks = list(scores.index) plt.barh(width, scores) plt.yticks(width, ticks) plt.title(\u0026#34;Mutual Information Scores\u0026#34;) plt.figure(dpi=100, figsize=(8, 5)) plot_mi_scores(mi_score) Your result should look like this:\nBased on the plot, we observe that the std_7, BAS, and Close features are the most important for predicting the Targets. On the other hand, the Open, Low, and High features do not provide significant information about the target variable. Therefore, we will remove these three features to avoid multicollinearity in the regression model using the drop function. df.drop(columns=[\u0026#39;Open\u0026#39;, \u0026#39;Low\u0026#39;, \u0026#39;High\u0026#39;], inplace=True) Let\u0026rsquo;s print the DataFrame df. If you follow the instructions, your DataFrame should look like this. As a result, we have only 1 dependent variable and 5 predictive features. Congratulation for passing this step.\n"
},
{
	"uri": "/3-deploy-model/",
	"title": "Deploy model",
	"tags": [],
	"description": "",
	"content": "Overall In this section, we will learn how to deploy the model for forecasting tasks. According to the ML process as shown below, we will focus on the Model deployment and predictions parts.\nContent Create API Configure docker container Test API Automate Inference "
},
{
	"uri": "/2-build-model/3-outlier-detection/",
	"title": "Outlier detection",
	"tags": [],
	"description": "",
	"content": "Outlier concept Outliers are data points that are significantly different from most of the other data points in a dataset. They can be the result of errors in data collection or can be genuine values that are different due to special circumstances.\nWhen building machine learning models, it may be necessary to remove outliers as they can negatively impact the model\u0026rsquo;s performance and accuracy. In this practice, we will learn about several methods for handling outliers.\nDetecting and handling outliers To start, we will visualize the target data using the plot() function, adjust the figsize parameter, and set a title for the plot. # VIsualizing target variable to eliminate outlier df[\u0026#39;Targets\u0026#39;].plot(figsize=(16,8), title=\u0026#39;Line chart of target variable\u0026#39;) Upon examining the results, we observe a data point that drops significantly below the -0.3 threshold at the beginning of 2020.\nNext, we will perform descriptive statistics on the Targets variable using the describe() function. #Finding the outlier value df[\u0026#39;Targets\u0026#39;].describe() The descriptive statistics show that the min and max values are quite far from the quartiles (25% and 75%).\nWe will create a new code line to create a box plot for the Targets series using the seaborn library. # Boxplot of Bitcoin return import seaborn as sns plt.figure(figsize=(10, 6)) # Create the boxplot sns.boxplot(y=df[\u0026#39;Targets\u0026#39;]) # Set the title and labels (optional) plt.title(\u0026#39;Boxplot of Bitcoin Returns\u0026#39;) plt.ylabel(\u0026#39;Daily Returns\u0026#39;) # Show the plot plt.show() A box plot helps us visualize the distribution of the data with its quartiles. Data points that lie outside the whiskers are considered outliers as they appear with low frequency and deviate significantly from the sample.\nIn this case, we observe a large number of outliers in the plot. However, there is one extremely low data point that is completely outside the data sample. Since Bitcoin has high price volatility, we might consider keeping the outliers that are closer to the majority of the values.\nTherefore, we will only remove the minimum value of the Targets variable and replace it with the mean value. The following code calculates the mean of Targets, finds the index of the minimum value, and replaces it in the DataFrame. #Replacing the outlier with mean value mean_return = df[\u0026#39;Targets\u0026#39;].mean() # Find the index of the minimum value in the return series min_index = df[\u0026#39;Targets\u0026#39;].idxmin() # Replace the minimum value with the mean value df.at[min_index, \u0026#39;Targets\u0026#39;] = mean_return We will replot the data series after removing the outlier using the plot() function as in the first step. #Return time series line chart without outlier df[\u0026#39;Targets\u0026#39;].plot(figsize=(16,8), title=\u0026#39;Line chart of target variable\u0026#39;) If you see that the outlier has been replaced as shown in the figure below, you have successfully completed this exercise.\n"
},
{
	"uri": "/3-deploy-model/3-test-api/",
	"title": "Test API",
	"tags": [],
	"description": "",
	"content": "Test API On the webpage created in the previous step, in the link field of the browser\u0026rsquo;s search bar, type /docs and press Enter. On the new webpage interface, click on the arrow next to the post section and then click on try it out. Under the Request body section, input the features data for forecasting in the form of a dictionary. Example: \u0026ldquo;features\u0026rdquo;: [64333.542969, 1.882768e+10, 0.018053, 0.014010, 0.026733]. Then click execute. If successful, the model\u0026rsquo;s forecast result for the provided input data will be displayed in the response body section. Congratulations on successfully completing this practice.\n"
},
{
	"uri": "/3-deploy-model/4-automate-inference/",
	"title": "Automate Inference",
	"tags": [],
	"description": "",
	"content": "Automate Inference In this workshop, we will create a program to automatically interact with the API, automatically send features data, and receive the prediction results.\nIn the TIME_SERIES directory, create a Python file named client.py. The following code is used to import the necessary libraries: json: a built-in module used for encoding and decoding JSON (JavaScript Object Notation) data. requests: a powerful and easy-to-use tool for sending HTTP requests. import json import requests We initialize a variable (data) to store the input, and a variable (url) to store the API endpoint. data = [[64333.542969, 1.882768e+10, 0.018053, 0.014010, 0.026733], [64178.992188, 2.143059e+10, 0.013952, 0.011276, 0.028999], [64094.355469, 4.253051e+10, 0.073014, 0.012506, 0.028688], [60381.914062, 2.762573e+10, 0.026280, 0.007034, 0.020180], [61175.191406, 3.273115e+10, 0.049908, 0.006023, 0.021483]] url = \u0026#39;http://localhost:8000/predict/\u0026#39; In practice, input data can be continuously retrieved and updated from a database. Here, we use an example input with features from any 5 days.\nNext, we will request the API to return the results as follows: Create a variable (predictions) as a list to store the results. Use a secondary variable (record) and a for loop to iterate through the input elements. Use another variable (payload) to convert each record into json format that the API can interact with. Use the requests library to make a post request to the API. Finally, use the append function to record the results into the predictions variable. predictions = [] for record in data: payload = {\u0026#39;features\u0026#39;: record} payload = json.dumps(payload) response = requests.post(url, data=payload) predictions.append(response.json()[\u0026#39;predicted_next_day_return\u0026#39;]) #Run code with interactive window print(predictions) Save the file and run it using the Interactive window in VSCode. The prediction results will be displayed in the interactive window as a list of output values from the API if the run is successful. In this workshop, you have gone through the steps to build a machine learning model and deploy it.\nSpecifically, we practiced techniques for handling time series data, working with the XGBoost model, building an API for the model, using tools like Jupyter Notebook, Docker, and more.\n"
},
{
	"uri": "/4-cleaning-resources/",
	"title": "Cleaning resources",
	"tags": [],
	"description": "",
	"content": "We will proceed to clean up resources in the following order:\nDelete the docker container by accessing Docker Desktop, going to container, clicking stop under the Actions section, and then clicking the delete icon next to it. Delete the image by navigating to the image tab, locating the image to delete, and clicking the delete icon next to it. "
},
{
	"uri": "/2-build-model/4-xgboost-model/",
	"title": "XGBoost model",
	"tags": [],
	"description": "",
	"content": "Training XGBoost Firstly, we\u0026rsquo;ll split the dataset into two parts: Train set and Validation set using the split function from the numpy library. We\u0026rsquo;ll use the first 80% of the data for model training and the remaining 20% for parameter tuning. Finally, we\u0026rsquo;ll print the shapes of both datasets using the shape function to verify. train_data, test_data = np.split(df, [int(0.8*len(df))]) print(train_data.shape, test_data.shape) To feed the data into the model, we\u0026rsquo;ll further split each dataset into two parts: X(containing the columns of the predictive variables) and y (containing the column of the target variable). The following code snippet splits the datasets using the pop function. y_train = train_data.pop(\u0026#39;Targets\u0026#39;) X_train = train_data y_valid = test_data.pop(\u0026#39;Targets\u0026#39;) X_valid = test_data We\u0026rsquo;ll import the XGBRegressor function from the xgboost library for model training and the mean_squared_error function from the scikit-learn library to calculate the error between predicted and actual values. from xgboost import XGBRegressor from sklearn.metrics import mean_squared_error Next, we\u0026rsquo;ll set the specific parameters of the XGBoost model using the XGBRegressor estimator and store the model in the variable model. objective: Specifies the loss function that the model needs to optimize. Here, we choose squared error for regression problems. n_estimators: Determines the number of base models in the XGBoost ensemble. learning_rate: Controls the contribution of each base model to the final model. early_stopping_rounds: Allows us to stop training early if the validation error doesn\u0026rsquo;t improve for a specified number of rounds, preventing overfitting. random_state: Ensures reproducibility. #Setting parameters model = XGBRegressor(objective=\u0026#39;reg:squarederror\u0026#39;, n_estimators=1000, learning_rate=0.05, early_stopping_rounds=5, random_state=0) We\u0026rsquo;ll train the model using the fit() function, passing the training data and specifying the validation set for evaluation. The verbose parameter controls the verbosity of the output. #Training model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],verbose =2) The training process for XGBoost will display a progress bar and evaluation metrics.\nCongratulations for completing training your XGBoost model.\n"
},
{
	"uri": "/2-build-model/5-model-evaluation/",
	"title": "Model evaluation",
	"tags": [],
	"description": "",
	"content": "Comparing with a benchmark model In this section, we will compare the results of our trained XGBoost model with the results of a baseline model. We will use a 7-day moving average model as our baseline.\nFirst, we will use the predict function to forecast the target variable values in the validation set and store these values in the predictions variable. Then, we use the sqrt function from the math library along with the mean_squared_error function to calculate the RMSE of the model\u0026rsquo;s predictions and print the result. import math # Root mean square error on validation set predictions = model.predict(X_valid) print(\u0026#34;Root mean squared error: \u0026#34; + str(math.sqrt(mean_squared_error(predictions, y_valid)))) The RMSE of the XGBoost model on the test dataset:\nNext, we calculate the error of the baseline model (MA_7) compared to the actual values using the same technique. Here, the moving average model has already been computed as the prediction variable, so there is no need to reinitialize the model. #Calculating error of moving average model on validation set rmse = math.sqrt(mean_squared_error(X_valid[\u0026#39;MA_7\u0026#39;], y_valid)) print(\u0026#34;Root mean squared error of moving average model: \u0026#34; + str(rmse)) The RMSE of the moving average model on the test dataset:\nThus, the XGBoost model performs better than the moving average model with a lower error value.\nVisualizing the predictions In addition to the error values, visualizing the forecast values will provide a more comprehensive view of the results.\nWe will use the familiar line chart to visualize the forecast data on the test dataset, stored in the predictions variable. Convert the predictions series to a pandas DataFrame, setting the index to be the index of the validation set. Rename the data column to prediction using the rename function. Use the plotly library with the line function to plot the predictions series, which will allow us to interact with the data on the graph. #Visualize insample prediction (validation_set) predictions = pd.DataFrame(predictions, index=test_data.index) predictions.rename(columns={0:\u0026#34;prediction\u0026#34;}, inplace=True) fig = px.line(predictions,y=predictions.columns, title=\u0026#39;Insample prediction (Validation set)\u0026#39;) fig.show() The visualization of the forecast data shows that most values are 0.00211, and the model also provides strong fluctuations at different times.\nSince the model\u0026rsquo;s results are better than the baseline model, we will save the model for deployment and operational use in necessary tasks. Using the joblib library, call the dump function, pass in our model variable model, and name the file as desired (e.g., model.joblib). import joblib joblib.dump(model, \u0026#39;model.joblib\u0026#39;) Perform a disk check and verify that the model has been saved correctly. Congratulations on completing the model evaluation practice.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]